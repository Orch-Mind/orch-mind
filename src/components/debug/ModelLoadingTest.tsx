// SPDX-License-Identifier: MIT OR Apache-2.0
// Copyright (c) 2025 Guilherme Ferrari Brescia

import React, { useState } from "react";
import { loadModelWithOptimalConfig } from "../../utils/transformersEnvironment";

interface ModelTestResult {
  success: boolean;
  message: string;
  details?: any;
  timestamp: string;
}

export const ModelLoadingTest: React.FC = () => {
  const [results, setResults] = useState<ModelTestResult[]>([]);
  const [isLoading, setIsLoading] = useState(false);

  const addResult = (result: Omit<ModelTestResult, "timestamp">) => {
    const newResult: ModelTestResult = {
      ...result,
      timestamp: new Date().toLocaleTimeString(),
    };
    setResults((prev) => [newResult, ...prev]);
    console.log(
      `[ModelTest] ${result.success ? "âœ…" : "âŒ"} ${result.message}`,
      result.details
    );
  };

  const testEnvironmentInitialization = async () => {
    try {
      addResult({
        success: true,
        message: "Testando inicializaÃ§Ã£o do ambiente transformers.js...",
      });

      // Teste 1: Verificar se transformers.js estÃ¡ disponÃ­vel
      try {
        addResult({
          success: true,
          message: "ğŸ” Verificando disponibilidade do transformers.js...",
        });

        const { pipeline, env } = await import("@huggingface/transformers");

        addResult({
          success: true,
          message: "âœ… Transformers.js importado com sucesso",
          details: {
            pipelineAvailable: typeof pipeline === "function",
            envAvailable: typeof env === "object",
          },
        });

        // Teste 2: Verificar configuraÃ§Ã£o do ambiente
        addResult({
          success: true,
          message: "ğŸ”§ Verificando configuraÃ§Ã£o do ambiente...",
        });

        addResult({
          success: true,
          message: `âœ… Ambiente configurado - Cache: ${
            env.cacheDir || "browser cache"
          }`,
          details: {
            cacheDir: env.cacheDir,
            backends: env.backends,
            allowLocalModels: env.allowLocalModels,
            allowRemoteModels: env.allowRemoteModels,
          },
        });

        // Teste 3: Verificar conectividade (opcional)
        addResult({
          success: true,
          message: "ğŸŒ Verificando conectividade com Hugging Face Hub...",
        });

        // Teste simples de conectividade
        try {
          const response = await fetch(
            "https://huggingface.co/api/models/Xenova/llama2.c-stories15M",
            {
              method: "HEAD",
              mode: "cors",
            }
          );

          if (response.ok) {
            addResult({
              success: true,
              message: "âœ… Conectividade com Hugging Face Hub OK",
            });
          } else {
            addResult({
              success: false,
              message: `âš ï¸ Conectividade limitada (status: ${response.status})`,
              details: {
                status: response.status,
                statusText: response.statusText,
              },
            });
          }
        } catch (fetchError) {
          addResult({
            success: false,
            message: "âš ï¸ Erro de conectividade - modelos podem nÃ£o carregar",
            details: {
              error:
                fetchError instanceof Error
                  ? fetchError.message
                  : String(fetchError),
              suggestion: "Verifique sua conexÃ£o com a internet",
            },
          });
        }
      } catch (importError) {
        addResult({
          success: false,
          message: "âŒ Erro na importaÃ§Ã£o do transformers.js",
          details: {
            error:
              importError instanceof Error
                ? importError.message
                : String(importError),
            stack: importError instanceof Error ? importError.stack : undefined,
            suggestions: [
              "Verifique se @huggingface/transformers estÃ¡ instalado",
              "Execute: npm install @huggingface/transformers",
              "Verifique se nÃ£o hÃ¡ conflitos de dependÃªncias",
            ],
          },
        });
        return;
      }

      addResult({
        success: true,
        message: "ğŸ‰ Ambiente inicializado com sucesso!",
      });
    } catch (error) {
      addResult({
        success: false,
        message: "âŒ Erro na inicializaÃ§Ã£o do ambiente",
        details: {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : undefined,
        },
      });
    }
  };

  const testSmallModelLoading = async () => {
    try {
      addResult({
        success: true,
        message: "Testando carregamento de modelo pequeno (tokenizer)...",
      });

      // Teste com um modelo muito pequeno primeiro - usar modelo ultra-pequeno para navegador
      const modelId = "Xenova/llama2.c-stories15M"; // Modelo de ~15MB, muito confiÃ¡vel para navegador
      const task = "text-generation";

      addResult({
        success: true,
        message: `Iniciando carregamento: ${modelId} (~15MB)`,
      });

      try {
        const model = await loadModelWithOptimalConfig(modelId, task, {
          // ConfiguraÃ§Ã£o especÃ­fica para modelos tiny
          dtype: "fp32", // Usar fp32 para modelos tiny (mais compatÃ­vel)
          device: "wasm", // ForÃ§ar WASM para compatibilidade mÃ¡xima
          use_external_data_format: false, // Modelos tiny geralmente nÃ£o precisam
          local_files_only: false, // Permitir download
        });

        if (model) {
          addResult({
            success: true,
            message: `Modelo ${modelId} carregado com sucesso!`,
            details: {
              modelType: typeof model,
              hasGenerate: typeof model.generate === "function",
              modelInfo: model.constructor?.name || "Unknown",
            },
          });

          // Teste bÃ¡sico de geraÃ§Ã£o com modelo tiny
          try {
            const testInput = "Hello";
            addResult({
              success: true,
              message: `Testando geraÃ§Ã£o com input: "${testInput}"`,
            });

            // Teste real de geraÃ§Ã£o com modelo tiny
            const result = await model(testInput, {
              max_new_tokens: 3,
              do_sample: false,
              temperature: 0.1,
            });

            addResult({
              success: true,
              message: `âœ… GeraÃ§Ã£o funcionou! Output: ${JSON.stringify(
                result
              ).substring(0, 100)}...`,
            });
          } catch (genError) {
            addResult({
              success: false,
              message: "Erro no teste de geraÃ§Ã£o",
              details: {
                error:
                  genError instanceof Error
                    ? genError.message
                    : String(genError),
                stack: genError instanceof Error ? genError.stack : undefined,
              },
            });
          }
        } else {
          addResult({
            success: false,
            message: "Modelo retornado Ã© null",
            details: { modelId, task },
          });
        }
      } catch (loadError) {
        addResult({
          success: false,
          message: `Erro especÃ­fico no carregamento do modelo ${modelId}`,
          details: {
            error:
              loadError instanceof Error
                ? loadError.message
                : String(loadError),
            stack: loadError instanceof Error ? loadError.stack : undefined,
            modelId,
            task,
          },
        });
      }
    } catch (error) {
      addResult({
        success: false,
        message: "Erro geral no carregamento do modelo pequeno",
        details: {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : undefined,
        },
      });
    }
  };

  const testTargetModelLoading = async () => {
    try {
      addResult({
        success: true,
        message:
          "Testando carregamento de modelos ultra-pequenos para navegador...",
      });

      // Lista de modelos ultra-pequenos que funcionam no navegador
      const tinyModels = [
        {
          id: "Xenova/llama2.c-stories15M",
          size: "~15MB",
          description: "Llama2.c Stories (ultra pequeno)",
          config: {
            dtype: "fp32" as const,
            device: "wasm" as const,
            use_external_data_format: false,
          },
        },
        {
          id: "Xenova/distilgpt2",
          size: "~353MB",
          description: "DistilGPT-2 otimizado",
          config: {
            dtype: "fp32" as const,
            device: "wasm" as const,
            use_external_data_format: false,
          },
        },
        {
          id: "Xenova/gpt2",
          size: "~548MB",
          description: "GPT-2 base (mÃ©dio)",
          config: {
            dtype: "fp32" as const,
            device: "wasm" as const,
            use_external_data_format: true,
          },
        },
        {
          id: "Xenova/TinyLlama-1.1B-Chat-v1.0",
          size: "~1.1B",
          description: "TinyLlama Chat (grande)",
          config: {
            dtype: "fp32" as const,
            device: "wasm" as const,
            use_external_data_format: true,
          },
        },
      ];

      for (const modelInfo of tinyModels) {
        try {
          addResult({
            success: true,
            message: `ğŸ§ª Testando ${modelInfo.id} (${modelInfo.size}) - ${modelInfo.description}`,
          });

          addResult({
            success: true,
            message: `ğŸ“¦ Importando transformers.js...`,
          });

          const { pipeline } = await import("@huggingface/transformers");

          addResult({
            success: true,
            message: `âœ… Transformers.js importado com sucesso`,
          });

          // Para modelos de texto, usar text-generation
          const task = modelInfo.id.includes("distilbert")
            ? "feature-extraction"
            : "text-generation";

          addResult({
            success: true,
            message: `ğŸ“¦ Criando pipeline ${task} para ${modelInfo.id}...`,
          });

          // Usar configuraÃ§Ã£o especÃ­fica para cada modelo
          const generator = await pipeline(task, modelInfo.id, {
            ...modelInfo.config,
            local_files_only: false, // Permitir download
            progress_callback: (data: any) => {
              if (data.status === "downloading") {
                addResult({
                  success: true,
                  message: `ğŸ“¥ Baixando: ${
                    data.name || data.file
                  } - ${Math.round(data.progress || 0)}%`,
                });
              }
            },
          } as any);

          addResult({
            success: true,
            message: `âœ… ${modelInfo.id} carregado com sucesso!`,
            details: {
              modelType: typeof generator,
              task: task,
              modelInfo: generator.constructor?.name || "Unknown",
              config: modelInfo.config,
            },
          });

          // Teste bÃ¡sico dependendo do tipo de modelo
          if (task === "text-generation") {
            addResult({
              success: true,
              message: `ğŸ§ª Testando geraÃ§Ã£o de texto...`,
            });

            const testOutput = await generator("Hello world", {
              max_new_tokens: 5,
              do_sample: false,
            });

            addResult({
              success: true,
              message: `ğŸ‰ GeraÃ§Ã£o funcionou! ${modelInfo.id}: ${JSON.stringify(
                testOutput
              ).substring(0, 80)}...`,
              details: {
                fullOutput: testOutput,
                outputType: typeof testOutput,
                outputLength: Array.isArray(testOutput) ? testOutput.length : 1,
              },
            });
          } else {
            // Para feature-extraction, apenas testar se carrega
            addResult({
              success: true,
              message: `âœ… ${modelInfo.id} pronto para feature extraction!`,
              details: {
                task: task,
                ready: true,
              },
            });
          }

          // Parar no primeiro modelo que funcionar
          addResult({
            success: true,
            message: `ğŸ¯ SUCESSO! Modelo ${modelInfo.id} estÃ¡ funcionando perfeitamente no navegador!`,
          });
          return;
        } catch (modelError) {
          addResult({
            success: false,
            message: `âŒ ${modelInfo.id} falhou: ${
              modelError instanceof Error
                ? modelError.message
                : String(modelError)
            }`,
            details: {
              error:
                modelError instanceof Error
                  ? modelError.message
                  : String(modelError),
              stack: modelError instanceof Error ? modelError.stack : undefined,
              modelId: modelInfo.id,
              modelSize: modelInfo.size,
              config: modelInfo.config,
            },
          });

          // Continuar para o prÃ³ximo modelo
          continue;
        }
      }

      // Se chegou aqui, nenhum modelo funcionou
      addResult({
        success: false,
        message:
          "âŒ Nenhum dos modelos ultra-pequenos funcionou. Problema pode ser de configuraÃ§Ã£o do ambiente.",
        details: {
          testedModels: tinyModels.map((m) => m.id),
          suggestions: [
            "Verifique se o transformers.js estÃ¡ instalado corretamente",
            "Limpe o cache do navegador",
            "Verifique a conexÃ£o com a internet",
            "Tente recarregar a pÃ¡gina",
            "Verifique o console do navegador para erros detalhados",
          ],
        },
      });
    } catch (error) {
      addResult({
        success: false,
        message: "âŒ Erro geral no teste de modelos pequenos",
        details: {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : undefined,
          suggestions: [
            "Problema pode ser na importaÃ§Ã£o do transformers.js",
            "Verifique se todas as dependÃªncias estÃ£o instaladas",
            "Confirme se o ambiente estÃ¡ configurado corretamente",
            "Verifique se hÃ¡ bloqueios de CORS ou rede",
          ],
        },
      });
    }
  };

  const runAllTests = async () => {
    setIsLoading(true);
    setResults([]);

    try {
      addResult({
        success: true,
        message: "ğŸš€ Iniciando todos os testes de modelo...",
      });

      // Teste 1: InicializaÃ§Ã£o do ambiente
      await testEnvironmentInitialization();

      // Aguardar um pouco entre testes
      await new Promise((resolve) => setTimeout(resolve, 1000));

      // Teste 2: Modelo pequeno primeiro
      await testSmallModelLoading();

      // Aguardar um pouco entre testes
      await new Promise((resolve) => setTimeout(resolve, 2000));

      // Teste 3: Modelo target
      await testTargetModelLoading();

      addResult({
        success: true,
        message: "ğŸ‰ Todos os testes de modelo concluÃ­dos!",
      });
    } catch (error) {
      addResult({
        success: false,
        message: "Erro geral nos testes",
        details: error instanceof Error ? error.message : String(error),
      });
    } finally {
      setIsLoading(false);
    }
  };

  const clearResults = () => {
    setResults([]);
  };

  const clearStoredModel = () => {
    // Limpar qualquer modelo armazenado e forÃ§ar o uso do tiny-gpt2
    const {
      setOption,
      getOption,
      STORAGE_KEYS,
    } = require("../../services/StorageService");

    const currentModel = getOption(STORAGE_KEYS.HF_MODEL);
    addResult({
      success: true,
      message: `ğŸ—‘ï¸ Modelo atual no storage: ${currentModel || "nenhum"}`,
    });

    // Limpar o storage e definir o modelo tiny
    setOption(STORAGE_KEYS.HF_MODEL, "Xenova/llama2.c-stories15M");

    addResult({
      success: true,
      message: `âœ… Modelo definido para: Xenova/llama2.c-stories15M`,
    });

    addResult({
      success: true,
      message: `ğŸ’¡ Agora execute o teste novamente para usar o modelo correto`,
    });
  };

  return (
    <div className="p-6 max-w-4xl mx-auto">
      <h2 className="text-2xl font-bold mb-4 text-white">
        ğŸ¤– Teste de Carregamento de Modelos
      </h2>

      <div className="flex gap-2 mb-4">
        <button
          onClick={runAllTests}
          disabled={isLoading}
          className="px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed"
        >
          {isLoading ? "â³ Executando..." : "ğŸš€ Executar Todos os Testes"}
        </button>

        <button
          onClick={testEnvironmentInitialization}
          disabled={isLoading}
          className="px-4 py-2 bg-green-600 text-white rounded hover:bg-green-700 disabled:opacity-50"
        >
          ğŸ”§ Testar Ambiente
        </button>

        <button
          onClick={testSmallModelLoading}
          disabled={isLoading}
          className="px-4 py-2 bg-yellow-600 text-white rounded hover:bg-yellow-700 disabled:opacity-50"
        >
          ğŸ“¦ Testar Modelo Pequeno
        </button>

        <button
          onClick={testTargetModelLoading}
          disabled={isLoading}
          className="px-4 py-2 bg-purple-600 text-white rounded hover:bg-purple-700 disabled:opacity-50"
        >
          ğŸ§ª Testar Modelos Tiny
        </button>

        <button
          onClick={clearResults}
          className="px-4 py-2 bg-gray-600 text-white rounded hover:bg-gray-700"
        >
          ğŸ—‘ï¸ Limpar
        </button>

        <button
          onClick={clearStoredModel}
          className="px-4 py-2 bg-gray-600 text-white rounded hover:bg-gray-700"
        >
          ğŸ—‘ï¸ Limpar Modelo Armazenado
        </button>
      </div>

      <div className="bg-gray-900 rounded-lg p-4 h-96 overflow-y-auto">
        <h3 className="text-lg font-semibold mb-2 text-white">
          Resultados dos Testes:
        </h3>

        {results.length === 0 ? (
          <p className="text-gray-400">
            Nenhum teste executado ainda. Clique em um botÃ£o acima para comeÃ§ar.
          </p>
        ) : (
          <div className="space-y-2">
            {results.map((result, index) => (
              <div
                key={index}
                className={`p-3 rounded border-l-4 ${
                  result.success
                    ? "bg-green-900/30 border-green-500 text-green-100"
                    : "bg-red-900/30 border-red-500 text-red-100"
                }`}
              >
                <div className="flex justify-between items-start">
                  <span className="font-medium">
                    {result.success ? "âœ…" : "âŒ"} {result.message}
                  </span>
                  <span className="text-xs opacity-70">{result.timestamp}</span>
                </div>

                {result.details && (
                  <details className="mt-2">
                    <summary className="cursor-pointer text-sm opacity-80 hover:opacity-100">
                      Detalhes
                    </summary>
                    <pre className="mt-1 text-xs bg-black/30 p-2 rounded overflow-x-auto">
                      {typeof result.details === "string"
                        ? result.details
                        : JSON.stringify(result.details, null, 2)}
                    </pre>
                  </details>
                )}
              </div>
            ))}
          </div>
        )}
      </div>

      <div className="mt-4 text-sm text-gray-400">
        <p>
          ğŸ’¡ <strong>Dica:</strong> Execute "Testar Ambiente" primeiro, depois
          "Testar Modelo Pequeno" para verificar se tudo funciona antes de
          tentar os modelos tiny ultra-compatÃ­veis.
        </p>
        <p>
          ğŸ” <strong>Debug:</strong> Verifique o console do navegador para logs
          detalhados durante o carregamento.
        </p>
        <p>
          ğŸ§ª <strong>Modelos Testados:</strong> Xenova/llama2.c-stories15M
          (~15MB), Xenova/distilgpt2 (~353MB), Xenova/gpt2 (~548MB),
          Xenova/TinyLlama-1.1B-Chat-v1.0 (~1.1B)
        </p>
      </div>
    </div>
  );
};

export default ModelLoadingTest;
