version: "3.8"

# vLLM + Ollama Integration Docker Compose
# Based on official vLLM documentation: https://docs.vllm.ai/en/v0.8.4/deployment/docker.html
# References:
# - https://medium.com/@kimdoil1211/effortless-vllm-deployment-with-docker-a-comprehensive-guide-2a23119839e2
# - https://dasarpai.com/dsblog/integrating-ollama-and-open-webui-with-docker

services:
  # Ollama service for model management and hosting
  ollama:
    image: ollama/ollama:latest
    container_name: orch-ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      # Persistent model storage
      - ollama_models:/root/.ollama/models
      - ollama_config:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - orch-ai-network
    # GPU support for NVIDIA cards
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # vLLM OpenAI-compatible API server (following official docs)
  vllm:
    image: vllm/vllm-openai:latest # Official vLLM image
    container_name: orch-vllm
    runtime: nvidia # Required for GPU support per documentation
    ports:
      - "${VLLM_PORT:-33220}:8000"
    volumes:
      # HuggingFace cache (required by official docs)
      - ~/.cache/huggingface:/root/.cache/huggingface
      # Share models with Ollama
      - ollama_models:/models:ro
    environment:
      # HuggingFace token for private models
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=0
    # Use --ipc=host for PyTorch shared memory (critical per docs)
    ipc: host
    # Alternative: use shm_size if ipc:host is not desired
    # shm_size: 16gb
    command: >
      --model ${VLLM_MODEL_NAME:-microsoft/DialoGPT-medium}
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --dtype auto
      --max-model-len ${VLLM_MAX_MODEL_LEN:-4096}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION:-0.9}
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - orch-ai-network
    # GPU support following official docs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Redis for caching and session management (optional)
  redis:
    image: redis:7-alpine
    container_name: orch-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-orchos123}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - orch-ai-network
    profiles:
      - full

  # Monitoring with Prometheus (optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: orch-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=200h"
      - "--web.enable-lifecycle"
    restart: unless-stopped
    networks:
      - orch-ai-network
    profiles:
      - monitoring

  # Grafana for visualization (optional)
  grafana:
    image: grafana/grafana:latest
    container_name: orch-grafana
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-orchos123}
      - GF_USERS_ALLOW_SIGN_UP=false
    restart: unless-stopped
    depends_on:
      - prometheus
    networks:
      - orch-ai-network
    profiles:
      - monitoring

# Persistent volumes for data storage
volumes:
  ollama_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${OLLAMA_MODELS_PATH:-./data/ollama/models}
  ollama_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${OLLAMA_CONFIG_PATH:-./data/ollama/config}
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# Networks for service communication
networks:
  orch-ai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
